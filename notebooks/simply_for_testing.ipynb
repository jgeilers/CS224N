{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JlkyXbxTcUu6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614572684610,"user_tz":300,"elapsed":2063,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"a8c9394d-14a2-4641-c6dd-11206149894f"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7Z22qIv7Fgd","executionInfo":{"status":"ok","timestamp":1614572684965,"user_tz":300,"elapsed":2405,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"e5a5d91c-dec3-4088-9d74-68e155551a09"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LGZLrWK7FeB","executionInfo":{"status":"ok","timestamp":1614572687349,"user_tz":300,"elapsed":4777,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"c5d682c0-0c13-46b2-c209-2af9e6b7aa55"},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dDlK3-2i7Fbh","executionInfo":{"status":"ok","timestamp":1614572689541,"user_tz":300,"elapsed":6957,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"2b24dbba-d381-41f5-972d-921217677f38"},"source":["!pip install wget"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMiJWk8b7FY2","executionInfo":{"status":"ok","timestamp":1614572689542,"user_tz":300,"elapsed":6947,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"4a4649f1-64c4-4f70-fcfc-e28caf0f21be"},"source":["import wget\n","import os\n","\n","print('Downloading dataset...')\n","\n","# The URL for the dataset zip file.\n","url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n","\n","# Download the file (if we haven't already)\n","if not os.path.exists('./cola_public_1.1.zip'):\n","    wget.download(url, './cola_public_1.1.zip')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading dataset...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iDJazKQ77FWZ","executionInfo":{"status":"ok","timestamp":1614572689542,"user_tz":300,"elapsed":6939,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":["# Unzip the dataset (if we haven't already)\n","if not os.path.exists('./cola_public/'):\n","    !unzip cola_public_1.1.zip"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"TyfCg2fR7FUS","executionInfo":{"status":"ok","timestamp":1614572689636,"user_tz":300,"elapsed":7023,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"64918c4e-401c-4f16-8e3b-850cea6614c6"},"source":["import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n","\n","# Report the number of sentences.\n","print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data.\n","df.sample(10)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Number of training sentences: 8,551\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_source</th>\n","      <th>label</th>\n","      <th>label_notes</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1753</th>\n","      <td>r-67</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>The more contented I laughed at the nurse who ...</td>\n","    </tr>\n","    <tr>\n","      <th>2630</th>\n","      <td>l-93</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Doug cleared the table.</td>\n","    </tr>\n","    <tr>\n","      <th>1974</th>\n","      <td>r-67</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Tom will force you to marry no student, and ne...</td>\n","    </tr>\n","    <tr>\n","      <th>6326</th>\n","      <td>c_13</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>The army's destruction of the palace was a tra...</td>\n","    </tr>\n","    <tr>\n","      <th>2223</th>\n","      <td>l-93</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>That dog bites people.</td>\n","    </tr>\n","    <tr>\n","      <th>4259</th>\n","      <td>ks08</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Mary persuaded John to fix the computer.</td>\n","    </tr>\n","    <tr>\n","      <th>822</th>\n","      <td>bc01</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>John persuaded Mary to kiss him.</td>\n","    </tr>\n","    <tr>\n","      <th>4494</th>\n","      <td>ks08</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>Lee is believed not to like Kim.</td>\n","    </tr>\n","    <tr>\n","      <th>4590</th>\n","      <td>ks08</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>He never achieved anything, didn't he?</td>\n","    </tr>\n","    <tr>\n","      <th>412</th>\n","      <td>bc01</td>\n","      <td>0</td>\n","      <td>*</td>\n","      <td>A man to come is unlikely.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     sentence_source  ...                                           sentence\n","1753            r-67  ...  The more contented I laughed at the nurse who ...\n","2630            l-93  ...                            Doug cleared the table.\n","1974            r-67  ...  Tom will force you to marry no student, and ne...\n","6326            c_13  ...  The army's destruction of the palace was a tra...\n","2223            l-93  ...                             That dog bites people.\n","4259            ks08  ...           Mary persuaded John to fix the computer.\n","822             bc01  ...                   John persuaded Mary to kiss him.\n","4494            ks08  ...                   Lee is believed not to like Kim.\n","4590            ks08  ...             He never achieved anything, didn't he?\n","412             bc01  ...                         A man to come is unlikely.\n","\n","[10 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"-swxZKVG7FRu","executionInfo":{"status":"ok","timestamp":1614572689637,"user_tz":300,"elapsed":7012,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"74286135-5c3a-43ff-9ee8-b2232d1f9203"},"source":["df.loc[df.label == 0].sample(5)[['sentence', 'label']]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2848</th>\n","      <td>The stick touched the fence.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5293</th>\n","      <td>He's a man to whom as for liberty, we could ne...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5182</th>\n","      <td>I don't have almost any potatoes.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>749</th>\n","      <td>We proclaimed sincerely to the public John to ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3552</th>\n","      <td>He treats John very kind.</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               sentence  label\n","2848                       The stick touched the fence.      0\n","5293  He's a man to whom as for liberty, we could ne...      0\n","5182                  I don't have almost any potatoes.      0\n","749   We proclaimed sincerely to the public John to ...      0\n","3552                          He treats John very kind.      0"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"rWP2Apdn7FPD","executionInfo":{"status":"ok","timestamp":1614572689638,"user_tz":300,"elapsed":7003,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":["# Get the lists of sentences and their labels.\n","sentences = df.sentence.values\n","labels = df.label.values"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U-zjjd_S7FMk","executionInfo":{"status":"ok","timestamp":1614572689974,"user_tz":300,"elapsed":7330,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"96ff5a8f-9049-488e-dc1e-4872fe0465f7"},"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFWJvqqw7FIQ","executionInfo":{"status":"ok","timestamp":1614572689975,"user_tz":300,"elapsed":7321,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"9f963518-02bd-4c5f-9f35-d47e65b34203"},"source":["# Print the original sentence.\n","print(' Original: ', sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":11,"outputs":[{"output_type":"stream","text":[" Original:  Our friends won't buy this analysis, let alone the next one we propose.\n","Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n","Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiXzF62d7FE2","executionInfo":{"status":"ok","timestamp":1614572692283,"user_tz":300,"elapsed":9619,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"e65e1d87-c77b-4d32-b374-a0395c3017fb"},"source":["max_len = 0\n","\n","# For every sentence...\n","for sent in sentences:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Max sentence length:  47\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8ckw2D-BfY7","executionInfo":{"status":"ok","timestamp":1614572692284,"user_tz":300,"elapsed":9612,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"a06f9134-cb9f-4d81-b654-bbf1fd1389c8"},"source":["print(type(labels))\n","print(type(labels[0]))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","<class 'numpy.int64'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RS2tqkA07E0d","executionInfo":{"status":"ok","timestamp":1614572695033,"user_tz":300,"elapsed":12353,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}},"outputId":"a8bda413-80fd-4707-a91d-22a50f440426"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 64,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:  Our friends won't buy this analysis, let alone the next one we propose.\n","Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n","         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZEtN9lvF7ExK","executionInfo":{"status":"ok","timestamp":1614572695034,"user_tz":300,"elapsed":12347,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8xkp56h7Euk","executionInfo":{"status":"ok","timestamp":1614572695035,"user_tz":300,"elapsed":12341,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bgiSbAM7Er0","executionInfo":{"status":"ok","timestamp":1614572695036,"user_tz":300,"elapsed":12336,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ph0GyH6T7Epd","executionInfo":{"status":"ok","timestamp":1614572695036,"user_tz":300,"elapsed":12330,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":[""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OmqTOl87Em2","executionInfo":{"status":"ok","timestamp":1614572695037,"user_tz":300,"elapsed":12326,"user":{"displayName":"Jackson Graham Eilers","photoUrl":"","userId":"02727037273747586745"}}},"source":[""],"execution_count":14,"outputs":[]}]}